{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤링\n",
    "\n",
    "## 개요\n",
    "웹 크롤링(Web Crawling)은 웹 페이지의 데이터를 자동으로 수집하는 기술입니다. 웹 크롤링은 주로 웹 데이터 분석, 데이터 수집, 웹 스크래핑(Web Scraping)으로 활용됩니다. 이 강의에서는 파이썬을 사용하여 정적 페이지와 동적 페이지 크롤링을 다룹니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 웹 크롤링의 기본 개념 이해\n",
    "- 정적 페이지 크롤링 방법 습득\n",
    "- 동적 페이지 크롤링 방법 습득\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 섹션 1: 웹 크롤링의 기본 개념\n",
    "\n",
    "### 1.1 웹 크롤링의 정의\n",
    "- **웹 크롤링(Web Crawling)**: 웹 페이지를 자동으로 방문하여 데이터를 수집하는 기술\n",
    "- **웹 스크래핑(Web Scraping)**: 웹 페이지에서 특정 데이터를 추출하는 기술\n",
    "\n",
    "### 1.2 웹 크롤링의 구성 요소\n",
    "- **크롤러(Crawler)**: 웹 페이지를 방문하고 데이터를 수집하는 프로그램\n",
    "- **파서(Parser)**: 가져온 HTML 소스를 분석하여 필요한 데이터를 추출\n",
    "- **데이터 저장(Data Storage)**: 추출한 데이터를 구조화하여 저장 (CSV, Excel, 데이터베이스 등)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 섹션 2: 정적 페이지 크롤링\n",
    "\n",
    "### 2.1 정적 페이지의 개념과 특징\n",
    "\n",
    "- **정적 페이지**: 서버에서 클라이언트(웹 브라우저)로 전송될 때 이미 완전히 구성된 HTML 문서\n",
    "- **특징**:\n",
    "  - **변경되지 않는 콘텐츠**: 정적 페이지는 서버에 저장된 HTML 파일로, 요청할 때마다 동일한 내용을 반환합니다.\n",
    "  - **빠른 로딩 속도**: 정적 페이지는 단순한 HTML 파일이므로, 서버와 클라이언트 간의 데이터 전송이 빠릅니다.\n",
    "  - **서버 부하 적음**: 서버에서 별도의 처리를 필요로 하지 않으므로, 서버 부하가 적습니다.\n",
    "  - **보안**: 서버 측 스크립트가 없어 보안 취약점이 적습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 기사 본문 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 정적 페이지 크롤링 예제\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. URL 요청\n",
    "url = 'https://n.news.naver.com/mnews/article/047/0002432943?sid=101'\n",
    "response = requests.get(url)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    \n",
    "    # 2. HTML 파싱\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # 3. 기사의 본문 텍스트 추출\n",
    "    article_tag = soup.find('div', {'id': 'dic_area'})\n",
    "    if article_tag:\n",
    "        article_text = article_tag.get_text(separator=' ').strip()\n",
    "        \n",
    "        # 기사 본문 출력\n",
    "        print(\"기사 본문:\")\n",
    "        print(article_text)\n",
    "    else:\n",
    "        print(\"기사를 포함하는 'div' 태그를 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f'HTTP 요청 실패. 상태 코드: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 텍스트 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. URL 요청\n",
    "url = 'https://n.news.naver.com/mnews/article/047/0002432943?sid=101'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    \n",
    "    # 2. HTML 파싱\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # 3. 특정 영역의 텍스트 추출\n",
    "    article_tag = soup.find('article', {'id': 'dic_area'})\n",
    "    if article_tag:\n",
    "        article_text = article_tag.get_text()\n",
    "        \n",
    "        # 4. 단어 수 계산\n",
    "        word_to_count = \"정부\"\n",
    "        count = article_text.count(word_to_count)\n",
    "        \n",
    "        print(f\"'{word_to_count}' 단어 수: {count}\")\n",
    "    else:\n",
    "        print(\"지정된 'article' 태그를 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f'HTTP 요청 실패. 상태 코드: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 섹션 3: 동적 페이지 크롤링\n",
    "\n",
    "### 3.1 동적 페이지의 개념과 특징\n",
    "\n",
    "- **동적 페이지**: 서버에서 클라이언트의 요청에 따라 HTML을 생성하여 전송하는 페이지\n",
    "- **특징**:\n",
    "    - 변경 가능한 콘텐츠: 동적 페이지는 사용자의 요청이나 입력에 따라 콘텐츠가 변경됩니다.\n",
    "    - 복잡한 로직 처리: 서버 측 스크립트를 사용하여 복잡한 로직을 처리하고, 데이터베이스와 상호작용할 수 있습니다.\n",
    "    - 느린 로딩 속도: 서버 측 스크립트를 실행하고 데이터베이스와 상호작용하는 데 시간이 필요하므로, 로딩 속도가 느릴 수 있습니다.\n",
    "    - 서버 부하 큼: 동적 페이지는 서버 측에서 많은 처리를 필요로 하므로, 서버 부하가 큽니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 정적 페이지와 동적 페이지의 차이\n",
    "\n",
    "| 특성              | 정적 페이지     | 동적 페이지     |\n",
    "|-------------------|----------------|----------------|\n",
    "| **콘텐츠 변경**   | 변경되지 않음   | 변경 가능       |\n",
    "| **로딩 속도**     | 빠름            | 느림            |\n",
    "| **서버 부하**     | 적음            | 큼              |\n",
    "| **복잡한 로직 처리** | 불가능         | 가능            |\n",
    "| **보안**          | 상대적으로 안전 | 보안 취약점 존재 |\n",
    "| **데이터베이스 사용** | 없음         | 있음            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 섹션 4: Selenium을 사용한 동적 페이지 크롤링\n",
    "\n",
    "- 셀레니움 설치 (pip install selenium)\n",
    "- 크롬 드라이버 설치 (https://googlechromelabs.github.io/chrome-for-testing/), 크롬 버전 확인\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Selenium을 사용한 크롤링의 동작 과정\n",
    "\n",
    "#### 1. URL 요청\n",
    "\n",
    "- Selenium은 웹 브라우저를 자동으로 제어하여 웹 페이지를 요청합니다.\n",
    "- `driver.get(URL)` 메서드를 사용하여 지정된 URL을 엽니다.\n",
    "\n",
    "#### 2. HTML 파싱\n",
    "\n",
    "- Selenium은 브라우저를 통해 로드된 웹 페이지의 HTML 소스를 가져와 이를 파싱할 수 있습니다.\n",
    "- `driver.page_source`를 사용하여 현재 페이지의 HTML 소스를 가져옵니다.\n",
    "\n",
    "#### 3. 데이터 추출\n",
    "\n",
    "- Selenium을 사용하여 찾은 요소에서 원하는 데이터를 추출합니다.\n",
    "- `find_element` 및 `find_elements` 메서드를 사용하여 특정 태그나 클래스 이름, ID 등을 가진 요소를 찾습니다.\n",
    "\n",
    "#### 4. 데이터 저장\n",
    "\n",
    "- 추출한 데이터를 원하는 형식으로 저장합니다.\n",
    "- CSV 파일, JSON 파일, 데이터베이스 등 다양한 형식으로 저장할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 동적 페이지 (네이버 기사 링크 수집)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Chrome 드라이버 경로 설정\n",
    "driver_path = 'C:/chromedriver-win64/chromedriver.exe'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# 웹사이트 열기\n",
    "url = 'https://search.naver.com/search.naver?where=news&query=%EB%B9%84%ED%8A%B8%EC%BD%94%EC%9D%B8&sm=tab_opt&sort=1&photo=0&field=0&pd=4&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Add%2Cp%3A1d&is_sug_officeid=0&office_category=0&service_area=0'  # 대상 웹사이트 URL로 변경\n",
    "driver.get(url)\n",
    "\n",
    "# 페이지 배율 줄이기 (줌 아웃)\n",
    "driver.execute_script(\"document.body.style.zoom='50%'\")\n",
    "time.sleep(2)  # 페이지 로드 시간을 위해 잠시 대기\n",
    "\n",
    "# 페이지 스크롤 끝까지 내리기\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# 데이터 수집\n",
    "news_data = []\n",
    "news_elements = driver.find_elements(By.CSS_SELECTOR, 'li.bx')\n",
    "\n",
    "for element in news_elements:\n",
    "    try:\n",
    "        link_element = element.find_element(By.CSS_SELECTOR, 'a[href*=\"https://n.news.naver.com/\"]')\n",
    "        link = link_element.get_attribute('href')\n",
    "        \n",
    "        # 제목 수집\n",
    "        title_element = element.find_element(By.CSS_SELECTOR, 'a.news_tit')\n",
    "        title = title_element.get_attribute('title')\n",
    "        \n",
    "        # 시간 수집 - span.info 요소에서 시간 텍스트 찾기\n",
    "        time_elements = element.find_elements(By.CSS_SELECTOR, 'span.info')\n",
    "        time_text = \"\"\n",
    "        for time_element in time_elements:\n",
    "            time_text = time_element.text\n",
    "            if \"분 전\" in time_text or \"시간 전\" in time_text or \"일 전\" in time_text:\n",
    "                break\n",
    "\n",
    "        news_data.append({\n",
    "            'title': title,\n",
    "            'time': time_text,\n",
    "            'link': link\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # 예외 발생 시 무시하고 다음 요소로 넘어감\n",
    "        continue\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 수집한 데이터를 CSV 파일로 저장\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "file_name = f'{current_time}.csv'\n",
    "\n",
    "with open(file_name, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['title', 'time', 'link'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "print(f'Data saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 이전에 수집한 기사 링크를 활용한 데이터 수집**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Chrome 드라이버 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "driver_path = 'C:/chromedriver-win64/chromedriver.exe'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 링크 목록을 저장한 CSV 파일 경로\n",
    "links_file = '0303.csv'  # 'links.csv' 파일은 현재 작업 디렉토리에 위치해야 합니다.\n",
    "\n",
    "# 링크 목록 읽기\n",
    "with open(links_file, mode='r', encoding='utf-8-sig') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    links = [{'title': row['title'], 'time': row['time'], 'link': row['link']} for row in reader]\n",
    "\n",
    "# 수집한 데이터를 저장할 리스트\n",
    "all_news_data = []\n",
    "\n",
    "for link_info in links:\n",
    "    driver.get(link_info['link'])\n",
    "    time.sleep(2)  # 페이지 로드 시간을 위해 잠시 대기\n",
    "\n",
    "    # 기사 제목 수집\n",
    "    title_element = driver.find_element(By.CSS_SELECTOR, 'h2#title_area span')\n",
    "    title = title_element.text\n",
    "\n",
    "    # 기사 입력 시간 수집\n",
    "    time_element = driver.find_element(By.CSS_SELECTOR, 'span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME')\n",
    "    article_time = time_element.getAttribute('data-date-time')\n",
    "\n",
    "    # 댓글 수 수집\n",
    "    comment_count_element = driver.find_element(By.CSS_SELECTOR, 'a#comment_count')\n",
    "    comment_count_text = comment_count_element.text\n",
    "    comment_count = 0 if 'is_zero' in comment_count_element.getAttribute('class') else int(comment_count_text)\n",
    "\n",
    "    if comment_count > 0:\n",
    "        # 댓글 페이지로 이동\n",
    "        comment_page_url = comment_count_element.getAttribute('href')\n",
    "        driver.get(comment_page_url)\n",
    "        \n",
    "        # 페이지 배율 줄이기 (줌 아웃)\n",
    "        driver.execute_script(\"document.body.style.zoom='50%'\")\n",
    "        time.sleep(2)  # 페이지 로드 시간을 위해 잠시 대기\n",
    "\n",
    "        # 클린봇 해제하여 모든 댓글을 볼 수 있도록 설정\n",
    "        try:\n",
    "            cleanbot = driver.find_element(By.CSS_SELECTOR, 'a.u_cbox_cleanbot_setbutton')\n",
    "            cleanbot.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            cleanbot_disable = driver.find_element(By.XPATH, \"//input[@id='cleanbot_dialog_checkbox_cbox_module']\")\n",
    "            cleanbot_disable.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            cleanbot_confirm = driver.find_element(By.CSS_SELECTOR, 'div.u_cbox_layer_cleanbot2_extra a')\n",
    "            cleanbot_confirm.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # \"더보기\" 버튼 클릭을 통한 댓글 로딩\n",
    "        while True:\n",
    "            try:\n",
    "                more_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.u_cbox_btn_more\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", more_button)\n",
    "                time.sleep(1)\n",
    "                more_button.click()\n",
    "                time.sleep(2)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                break\n",
    "            except ElementClickInterceptedException:\n",
    "                driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "                time.sleep(2)\n",
    "\n",
    "        # 데이터 수집\n",
    "        while True:\n",
    "            try:\n",
    "                comments = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_contents\")\n",
    "                comment_dates = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_date\")\n",
    "                recommend_counts = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_cnt_recomm\")\n",
    "                unrecommend_counts = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_cnt_unrecomm\")\n",
    "                break\n",
    "            except StaleElementReferenceException:\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        for i in range(len(comments)):\n",
    "            try:\n",
    "                comment = comments[i].text\n",
    "                comment_date = comment_dates[i].text\n",
    "                recommend_count = recommend_counts[i].text\n",
    "                unrecommend_count = unrecommend_counts[i].text\n",
    "\n",
    "                all_news_data.append({\n",
    "                    \"title\": title,\n",
    "                    \"time\": article_time,\n",
    "                    \"comment_count\": comment_count,\n",
    "                    \"comment\": comment,\n",
    "                    \"comment_date\": comment_date,\n",
    "                    \"recommend_count\": recommend_count,\n",
    "                    \"unrecommend_count\": unrecommend_count\n",
    "                })\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 수집한 데이터를 CSV 파일로 저장\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "file_name = f'{current_time}.csv'  # 파일을 현재 작업 디렉토리에 저장\n",
    "\n",
    "with open(file_name, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "    fieldnames = ['title', 'time', 'comment_count', 'comment', 'comment_date', 'recommend_count', 'unrecommend_count']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_news_data)\n",
    "\n",
    "print(f'Data saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **연습문제 1: 정적 페이지에서 텍스트 데이터 추출**\n",
    "\n",
    " - 주어진 URL의 페이지 제목을 추출하여 출력하는 코드를 작성하세요. URL은 https://example.com 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#해답:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL 요청\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "# HTML 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 제목 추출\n",
    "title = soup.title.string\n",
    "\n",
    "print(f\"페이지 제목: {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **연습문제 2: 요소 속성 설명**\n",
    "(https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query=%EC%9A%B0%EC%84%9D%EC%A7%84+%EA%B5%90%EC%88%98)\n",
    "- 뉴스 제목, 링크, 시간의 요소 속성을 기술하세요.\n",
    "    - 2.1 뉴스 제목 요소의 속성은 무엇입니까?\n",
    "    - 2.2 뉴스 링크 요소의 속성은 무엇입니까?\n",
    "    - 2.3 뉴스 시간 요소의 속성은 무엇입니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 뉴스 제목 요소의 속성: a.news_tit 태그의 title 속성\n",
    "- 뉴스 링크 요소의 속성: a[href*=\"https://n.news.naver.com/\"] 태그의 href 속성\n",
    "- 뉴스 시간 요소의 속성: span.info 태그의 텍스트 콘텐츠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **연습문제 3: 데이터 추출**\n",
    "- Selenium을 사용하여 주어진 URL의 뉴스 목록을 크롤링합니다.\n",
    "    - 3.1 각 뉴스 항목에서 뉴스 제목, 링크, 시간을 추출합니다.\n",
    "    - 3.2 추출한 데이터를 CSV 파일로 저장합니다.\n",
    "    - 3.3 CSV 파일의 필드명은 'title', 'link', 'time'입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Chrome 드라이버 설정\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# 웹사이트 열기\n",
    "url = 'https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query=%EC%9A%B0%EC%84%9D%EC%A7%84+%EA%B5%90%EC%88%98'\n",
    "driver.get(url)\n",
    "\n",
    "# 페이지 배율 줄이기 (줌 아웃)\n",
    "driver.execute_script(\"document.body.style.zoom='50%'\")\n",
    "time.sleep(2)\n",
    "\n",
    "# 페이지 스크롤 끝까지 내리기\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# 데이터 수집\n",
    "news_data = []\n",
    "news_elements = driver.find_elements(By.CSS_SELECTOR, 'li.bx')\n",
    "\n",
    "for element in news_elements:\n",
    "    try:\n",
    "        link_element = element.find_element(By.CSS_SELECTOR, 'a[href*=\"https://n.news.naver.com/\"]')\n",
    "        link = link_element.get_attribute('href')\n",
    "        \n",
    "        # 제목 수집\n",
    "        title_element = element.find_element(By.CSS_SELECTOR, 'a.news_tit')\n",
    "        title = title_element.get_attribute('title')\n",
    "        \n",
    "        # 시간 수집 - span.info 요소에서 시간 텍스트 찾기\n",
    "        time_elements = element.find_elements(By.CSS_SELECTOR, 'span.info')\n",
    "        time_text = \"\"\n",
    "        if len(time_elements) > 1:\n",
    "            time_text = time_elements[1].text\n",
    "        elif time_elements:\n",
    "            time_text = time_elements[0].text\n",
    "\n",
    "        news_data.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'time': time_text\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 수집한 데이터를 CSV 파일로 저장\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "file_name = f'{current_time}.csv'\n",
    "\n",
    "with open(file_name, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['title', 'link', 'time'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "print(f'Data saved to {file_name}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
