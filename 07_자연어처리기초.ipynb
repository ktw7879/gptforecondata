{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmlcks123/gptforecondata/blob/main/07_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%EA%B8%B0%EC%B4%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "zJ65q1xHlecn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 한국어 띄어쓰기 라이브러리 사전 설치(PyKoSpacing)\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "metadata": {
        "id": "WtMJOn7SmHB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0Ed4Z_9UiIQ"
      },
      "outputs": [],
      "source": [
        "## 한국어 전처리 라이브러리 사전설치 (KONLPY)\n",
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdn4atbRVHNa"
      },
      "source": [
        "질문 프롬프터\n",
        "\n",
        "1.\"주제: 자연어 처리 기초 세부주제: 텍스트 전처리, 토큰화, 임베딩\"에 대해서 2시간 가량 강의를 해야해\n",
        "\n",
        "강의안 만들어 줄 수 있어?\n",
        "\n",
        "2. 강의 목표와 목적에 대해서 만들어줘"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AFuA0PjJmi0H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xpSFhKCVuej"
      },
      "source": [
        "# 자연어 처리 기초 강의안\n",
        "\n",
        "## 목차\n",
        "1. **강의 소개 및 개요 **\n",
        "   - 자연어 처리(NLP)란?\n",
        "   - 강의 목적 및 목표\n",
        "\n",
        "2. **텍스트 전처리 **\n",
        "   - 개요 및 필요성\n",
        "   - 텍스트 정제 (텍스트 클리닝)\n",
        "     - 불필요한 문자 제거\n",
        "     - 대소문자 통일\n",
        "     - 정규 표현식 사용\n",
        "   - 텍스트 정규화\n",
        "     - 철자 수정\n",
        "     - 불용어 제거\n",
        "   - 텍스트 정규화 (예시 코드 포함)\n",
        "\n",
        "3. **토큰화 **\n",
        "   - 토큰화란?\n",
        "   - 단어 토큰화 vs. 문장 토큰화\n",
        "   - 토큰화 기법\n",
        "     - 띄어쓰기 기반 토큰화\n",
        "     - 형태소 분석 기반 토큰화\n",
        "   - 한국어 토큰화의 어려움과 해결 방안\n",
        "   - 토큰화 (예시 코드 포함)\n",
        "\n",
        "4. **텍스트 임베딩 **\n",
        "   - 텍스트 임베딩이란?\n",
        "   - 우리가 쓸꺼\n",
        "     - KONLPY\n",
        "     - PyKoSpacing\n",
        "\n",
        "5. **실습**\n",
        "\n",
        "   - 웹 크롤링(네이버 뉴스)\n",
        "   - Classify\n",
        "   - 언론사별 분류\n",
        "   - 단어 언급횟수 색인\n",
        "   - DataFrame화\n",
        "   - 불용어 추가하기\n",
        "   - Exel파일로 다운 받기\n",
        "\n",
        "6. **Q&A 및 마무리 **\n",
        "   - 질문 답변\n",
        "   - 강의 요약 및 마무리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHsYIzW5UnNb"
      },
      "source": [
        "# 자연어 처리란?\n",
        "\n",
        "자연어 처리(Natural Language Processing, NLP)는 인간의 언어를 컴퓨터가 이해하고 처리할 수 있도록 하는 기술입니다. NLP는 컴퓨터 과학, 인공지능, 언어학의 교차점에 위치한 분야로, 텍스트와 음성 데이터의 분석과 해석을 다룹니다. 자연어 처리는 검색 엔진, 번역기, 챗봇 등 다양한 애플리케이션에서 사용됩니다.\n",
        "\n",
        "## 1. 자연어 처리의 중요성\n",
        "\n",
        "- **정보 검색**: 검색 엔진이 사용자 쿼리에 맞는 결과를 제공\n",
        "- **번역 서비스**: 구글 번역과 같은 자동 번역 시스템\n",
        "- **챗봇과 가상 비서**: 고객 지원, 개인 비서 등의 역할\n",
        "- **감정 분석**: 소셜 미디어, 리뷰 사이트 등에서 감정 분석\n",
        "\n",
        "## 2. 자연어 처리의 주요 과제\n",
        "\n",
        "- **언어 모델링**: 텍스트 데이터를 기반으로 언어의 구조와 패턴을 학습\n",
        "- **문서 분류**: 텍스트 데이터를 카테고리로 분류\n",
        "- **개체명 인식 (NER)**: 텍스트에서 인물, 장소, 조직 등의 이름을 식별\n",
        "- **감정 분석**: 텍스트의 감정을 분석하여 긍정, 부정, 중립을 분류\n",
        "- **기계 번역**: 한 언어에서 다른 언어로 텍스트를 번역\n",
        "\n",
        "## 3. NLP의 주요 기술\n",
        "\n",
        "- **토큰화 (Tokenization)**: 텍스트를 단어 또는 문장 단위로 분리\n",
        "- **정규화 (Normalization)**: 대소문자 변환, 철자 수정 등 텍스트 일관성 유지\n",
        "- **형태소 분석 (Morphological Analysis)**: 단어의 형태를 분석하여 어근, 접사 등을 분리\n",
        "- **구문 분석 (Syntax Analysis)**: 문법 구조를 분석하여 구문 트리를 생성\n",
        "- **의미 분석 (Semantic Analysis)**: 단어와 문장의 의미를 분석\n",
        "\n",
        "## 4. 자연어 처리의 응용 분야\n",
        "\n",
        "- **정보 검색 (Information Retrieval)**\n",
        "- **질의 응답 시스템 (Question Answering)**\n",
        "- **챗봇 (Chatbots)**\n",
        "- **기계 번역 (Machine Translation)**\n",
        "- **음성 인식 (Speech Recognition)**\n",
        "- **감정 분석 (Sentiment Analysis)**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UBaoSEHXNDP"
      },
      "source": [
        "## 1. **강의 소개 및 개요**\n",
        "### 자연어 처리(NLP)란?\n",
        "앞서 설명한 바와 같이, 자연어 처리(NLP)는 인간의 언어를 컴퓨터가 이해하고 처리할 수 있도록 하는 기술입니다. 이제 우리는 이 기술의 실제 적용과 관련된 기초 개념을 배우고 실습해볼 것입니다.\n",
        "\n",
        "### 강의 목적 및 목표\n",
        "이번 강의의 목적과 목표는 다음과 같습니다:\n",
        "\n",
        "#### 목적\n",
        "- **자연어 처리의 기본 개념 이해**: NLP의 기초 개념과 필요성을 이해하고, 텍스트 데이터를 다루는 방법을 학습합니다.\n",
        "- **실습을 통한 경험 축적**: 다양한 NLP 기법을 실제 데이터에 적용해보며 실습을 통해 경험을 쌓습니다.\n",
        "- **실제 데이터 활용**: 원하는 키워드에 대한 기사를 가져와서 언론사별로 어떻게 묘사했는지 봅니다.\n",
        "\n",
        "#### 목표\n",
        "- **텍스트 전처리**: 텍스트 데이터를 정제하고 정규화하는 방법을 배웁니다.\n",
        "- **토큰화**: 텍스트를 토큰으로 분리하는 다양한 기법을 이해하고 적용합니다.\n",
        "- **텍스트 임베딩**: 텍스트 데이터를 벡터로 변환하는 다양한 임베딩 기법을 학습합니다.\n",
        "- **도구 활용**: KONLPY와 PORORO 등 주요 NLP 도구를 사용하여 실제 데이터를 처리합니다.\n",
        "- **데이터 분석**: 키워드에 따른 기사에서 사용된 단어들을 분석하고, 이를 전처리하는 것을 연습합니다.\n",
        "이 강의를 통해 여러분은 자연어 처리의 기초를 탄탄히 다지고, 실습을 통해 이를 실제 문제에 적용할 수 있는 능력을 키울 것입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fob9r-e_YBkz"
      },
      "source": [
        "## 2. **텍스트 전처리**\n",
        "\n",
        "### 개요 및 필요성\n",
        "텍스트 전처리는 자연어 처리(NLP)에서 필수적인 첫 단계입니다. 원본 텍스트 데이터는 불필요한 기호, 오탈자, 비정형 데이터 등으로 인해 바로 분석하거나 모델링에 사용하기 어렵습니다. 따라서 텍스트 전처리를 통해 데이터를 정제하고 정규화하는 과정이 필요합니다.\n",
        "\n",
        "#### 전처리의 필요성\n",
        "- **데이터의 일관성 유지**: 정제된 데이터는 일관성을 가지며, 모델의 성능을 향상시킵니다.\n",
        "- **노이즈 제거**: 불필요한 문자와 기호를 제거하여 데이터의 품질을 높입니다.\n",
        "- **분석 용이성**: 정규화된 데이터는 분석과 모델링에 적합합니다.\n",
        "- **정확한 모델 훈련**: 전처리된 데이터는 모델 훈련 시 더 나은 성능을 보입니다.\n",
        "\n",
        "### 텍스트 정제 (텍스트 클리닝)\n",
        "\n",
        "#### 불필요한 문자 제거\n",
        "텍스트 데이터에는 분석에 불필요한 특수 문자, 숫자, 기호 등이 포함될 수 있습니다. 이러한 문자를 제거하여 데이터의 일관성을 높입니다.\n",
        "\n",
        "### 텍스트 정규화\n",
        "\n",
        "#### 철자 수정\n",
        "텍스트 데이터에는 오탈자가 있을 수 있습니다. 이를 수정하여 데이터의 품질을 높입니다.\n",
        "\n",
        "#### 불용어 제거\n",
        "불용어(Stop Words)는 분석에 큰 의미가 없는 단어들입니다. 예를 들어, 'is', 'and', 'the', 한국어, '에', '에게', '이다.' 등이 불용어에 해당합니다. 이를 제거하여 중요한 단어들만 남깁니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. **토큰화**\n",
        "\n",
        "### 토큰화란?\n",
        "토큰화(Tokenization)는 텍스트를 작은 단위인 토큰(Token)으로 분리하는 과정입니다. 토큰은 단어, 문장, 구두점 등의 개별 요소를 나타낼 수 있습니다. 토큰화는 자연어 처리의 기본 단계 중 하나로, 텍스트 데이터를 분석하기 위한 필수 과정입니다.\n",
        "\n",
        "### 단어 토큰화 vs. 문장 토큰화\n",
        "- **단어 토큰화**: 텍스트를 단어 단위로 분리합니다.\n",
        "- **문장 토큰화**: 텍스트를 문장 단위로 분리합니다.\n",
        "\n",
        "### 토큰화 기법\n",
        "토큰화에는 다양한 기법이 존재하며, 텍스트의 특성과 분석 목적에 따라 적합한 기법을 선택해야 합니다.\n",
        "\n",
        "#### 띄어쓰기 기반 토큰화\n",
        "띄어쓰기를 기준으로 텍스트를 분리합니다. 간단하지만 언어에 따라 정확도가 떨어질 수 있습니다.\n",
        "\n",
        "#### 형태소 분석 기반 토큰화\n",
        "형태소(Morpheme)란 의미를 가지는 가장 작은 단위입니다. 형태소 분석을 통해 단어를 더 작은 단위로 분리할 수 있습니다. 특히 한국어와 같은 교착어에서 유용합니다.\n",
        "\n",
        "### 한국어 토큰화의 어려움과 해결 방안\n",
        "한국어는 영어와 달리 교착어이며, 형태소가 복잡하게 결합되는 특성을 가집니다. 따라서 단순한 띄어쓰기 기반 토큰화는 정확도가 떨어질 수 있습니다. 형태소 분석기(Okt, Komoran 등)를 사용하여 더 정확한 토큰화를 수행할 수 있습니다.\n",
        "\n",
        "### 토큰화 (예시 코드 포함)\n",
        "다양한 토큰화 기법을 적용해봅시다.\n",
        "\n",
        "토큰화를 통해 텍스트를 분석하기 용이한 단위로 분리할 수 있습니다. 다음 섹션에서는 텍스트 임베딩에 대해 알아보겠습니다.\n",
        "\n",
        "## 4. **텍스트 임베딩**\n",
        "\n",
        "### 텍스트 임베딩이란?\n",
        "텍스트 임베딩(Text Embedding)은 텍스트 데이터를 고정 길이의 벡터로 변환하는 과정입니다. 이를 통해 텍스트를 기계 학습 모델이 처리할 수 있는 형태로 만듭니다. 임베딩은 텍스트의 의미를 수치 벡터로 표현하여 단어 간의 유사도 계산을 가능하게 합니다.\n",
        "\n",
        "### 우리가 쓸 도구\n",
        "이번 강의에서는 한국어 처리를 위해 주로 사용하는 두 가지 도구를 소개합니다: KONLPY와 PORORO.\n",
        "\n",
        "#### KONLPY\n",
        "KONLPY는 한국어 자연어 처리를 위한 파이썬 패키지로, 다양한 형태소 분석기와 유틸리티를 제공합니다.\n",
        "\n",
        "#### PyKoSpacing\n",
        "PyKoSpacing 는 한국어의 띄어쓰기와 맞춤법을 교정해주는 패키지입니다.\n",
        "\n",
        "우리는 이번에 인터넷에서 크롤링한 뉴스 자연어 데이터를 바탕으로 PyKoSpacing를 이용해 먼저 정리 한 후 KoNLPy를 이용해 데이터를 전처리 하겠습니다."
      ],
      "metadata": {
        "id": "cRInnZjWZbfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 실습예제"
      ],
      "metadata": {
        "id": "cRWddKXSfcp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##만들어질 데이터 classify\n",
        "\n",
        "class News:\n",
        "  news_word_dictionary = {}\n",
        "\n",
        "  def __init__(self, name, url, title):\n",
        "    self.name = name\n",
        "    self.url = url\n",
        "    self.title = title\n",
        "\n",
        "## 뉴스 텍스트\n",
        "  def setNewsText(self, text):\n",
        "    self.news_text = text\n",
        "\n",
        "## 토큰화된 뉴스 텍스트\n",
        "  def setTokenizedText(self, tokenized_text):\n",
        "    self.tokenized_text = tokenized_text\n",
        "\n",
        "## 불용어 처리 후 뉴스 텍스트\n",
        "  def setFilteredText(self, filtered_text):\n",
        "    self.filtered_text = filtered_text\n",
        "\n",
        "## 사용할 텍스트\n",
        "  def setUseText(self, use_text):\n",
        "    self.use_text = use_text\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"[{self.name}]: {self.title}\\n - URL: {self.url}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "J0RoAyBN7Vmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR2NFv3IZKuK"
      },
      "outputs": [],
      "source": [
        "# 1. 인터넷 기사 크롤링\n",
        "\n",
        "# 1.0 변수 선언\n",
        "all_news_list = [] # 뉴스 인스턴스 리스트\n",
        "all_news_title = []\n",
        "all_news_name = []\n",
        "all_news_url = []\n",
        "\n",
        "# 1.1 키워드 검색\n",
        "import requests\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.parse import quote\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib3\n",
        "\n",
        "keyword = input(\"검색할 키워드를 입력해주세요. :\")\n",
        "encoded_keyword = quote(keyword)\n",
        "\n",
        "# 1.2 네이버 URL 검색 요청\n",
        "req = Request(f'https://search.naver.com/search.naver?sm=tab_hty.top&where=news&ssc=tab.news.all&query={encoded_keyword}', headers={'User-Agent': 'Mozilla/5.0'})\n",
        "webpage = urlopen(req).read()\n",
        "\n",
        "# 1.3 BeautifulSoup 객체 생성\n",
        "soup = BeautifulSoup(webpage, 'html.parser')\n",
        "\n",
        "# 1.4 지정된 셀렉터 추출: 네이버 검색 결과 리스트 --> bs4 객체화 후 추출\n",
        "news_section = soup.select('#main_pack > section > div.api_subject_bx > div.group_news > ul')\n",
        "\n",
        "# 1.5 section 태그 정보 추출\n",
        "for section in news_section:\n",
        "    # 1.5.1 뉴스 제목 & url 추출\n",
        "    titles = section.find_all('a', class_='news_tit')\n",
        "    for title in titles:\n",
        "        news_url = title.get(\"href\")\n",
        "        all_news_url.append(news_url)\n",
        "        all_news_title.append(title.get_text(strip=True))\n",
        "\n",
        "    # 1.5.2 언론사 이름 추출\n",
        "    a_tags = soup.find_all('a', class_='info press')\n",
        "\n",
        "    for a in a_tags:\n",
        "        news_name = a.get_text(strip=True)[:-6]\n",
        "        all_news_name.append(news_name)\n",
        "\n",
        "# 1.5.3 뉴스 인스턴스 생성\n",
        "for i in range(len(all_news_title)):\n",
        "  new_news = News(all_news_name[i], all_news_url[i], all_news_title[i])\n",
        "  print(new_news)\n",
        "  all_news_list.append(new_news)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 각 뉴스 별 텍스트 데이터 추출\n",
        "news_text = []\n",
        "\n",
        "for news in all_news_list:\n",
        "    # 2.1 뉴스 URL GET 요청\n",
        "    news_response = requests.get(news.url)\n",
        "\n",
        "    # 2.2 GET 요청 성공\n",
        "    if news_response.status_code == 200:\n",
        "        # 2.2.1 HTML 코드 --> bs4 객체화\n",
        "        news_html_content = news_response.text\n",
        "        news_soup = BeautifulSoup(news_html_content, 'html.parser')\n",
        "\n",
        "        # 2.2.2 body 내용 추출\n",
        "        body = news_soup.body\n",
        "\n",
        "        if body:\n",
        "            # script 태그 제거\n",
        "            for script in body(['script']):\n",
        "                script.extract()\n",
        "\n",
        "            # body 태그 안의 텍스트 추출\n",
        "            body_text = body.get_text(strip=True)\n",
        "            news.setNewsText(body_text)\n",
        "            print(f\"News's Body Text: {news.news_text}\")\n",
        "        else:\n",
        "            print(\"Body tag not found.\")"
      ],
      "metadata": {
        "id": "oCtJZSMk82bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp6_Kjb2ZbN7"
      },
      "outputs": [],
      "source": [
        "# 3. 기사 텍스트 데이터 전처리\n",
        "\n",
        "# 3.0 띄어쓰기 정규화\n",
        "from pykospacing import Spacing\n",
        "spacing = Spacing()\n",
        "\n",
        "for news_instance in all_news_list:\n",
        "    kospacing_text = spacing(news_instance.news_text)\n",
        "    news_instance.setNewsText(kospacing_text)\n",
        "    print(news_instance.news_text)\n",
        "    print(kospacing_text)\n",
        "\n",
        "# 3.1 토큰화\n",
        "from konlpy.tag import Okt\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "okt = Okt()\n",
        "tokenized_text = []\n",
        "news_text = []\n",
        "\n",
        "for news_instance in all_news_list:\n",
        "    tokenized_text = okt.morphs(news_instance.news_text)\n",
        "    for token in tokenized_text:\n",
        "        news_text.append(token)\n",
        "\n",
        "    # 뉴스 인스턴스에 토크나이즈화 된 텍스트 입력\n",
        "    news_instance.setTokenizedText(tokenized_text)\n",
        "    print(\"Tokens:\", tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 4. 데이터 분석: 단어 카운팅\n",
        "\n",
        "# 4.1 all_word_list & all_news_name_list 생성\n",
        "all_word_set = set()\n",
        "all_news_name_set = set()\n",
        "\n",
        "for news_instance in all_news_list:\n",
        "  all_word_set.update(news_instance.tokenized_text)\n",
        "  all_news_name_set.add(news_instance.name)\n",
        "\n",
        "# 4.2 데이터 프레임 미리 생성\n",
        "df = pd.DataFrame(index=list(all_news_name_set), columns=list(all_word_set))\n",
        "df = df.fillna(0)  # 비어있는 셀을 0으로 채우기\n",
        "\n",
        "# 4.3 Counting 작업\n",
        "for news_instance in all_news_list:\n",
        "  # 단어 빈도수 계산\n",
        "  word_freq = Counter(news_instance.tokenized_text)\n",
        "\n",
        "  # 단어와 빈도수를 분리하여 그래프 데이터 준비\n",
        "  for word, count in word_freq.items():\n",
        "    if word in df.columns and news_instance.name in df.index:\n",
        "      df.at[news_instance.name, word] += count\n",
        "\n",
        "# 4.4 Excel 추출\n",
        "df.to_excel(\"./news_keyword_count.xlsx\", index=True)"
      ],
      "metadata": {
        "id": "4KTiF4QrCK-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 불용어 처리\n",
        "\n",
        "# 한국어 불용어 리스트\n",
        "korean_stopwords = set([\n",
        "        '의', '가', '이', '은', '들', '는', '과', '도', '를', '으로', '에', '와', '한', '하다'\n",
        "])\n",
        "\n",
        "# 불용어 제거\n",
        "filtered_text = [word for word in token if word not in korean_stopwords]\n",
        "print(\"Filtered Tokens:\", filtered_text)\n",
        "news_instance.setFilteredText(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY1IPMRNtHCP",
        "outputId": "cb8aaef7-d69c-4f3c-8186-ad82838015b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['+']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 변수만 사용\n",
        "use_words = set([\n",
        "        '트럼프', '대선', '총격'\n",
        "])\n",
        "\n",
        "use_text = [word for word in token if word in use_words]\n",
        "print(\"use Tokens:\", use_text)\n",
        "news_instance.setUseText(use_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F0CN4e2vm8l",
        "outputId": "de9d97da-e587-464b-95e5-dd894437e122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use Tokens: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4 Excel 추출\n",
        "df.to_excel(\"./news_keyword_count_1.xlsx\", index=True)"
      ],
      "metadata": {
        "id": "Sns5mBSwvbBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Q&A 및 마무리**\n",
        "\n",
        "### 강의 요약 및 마무리\n",
        "오늘 강의에서 배운 주요 내용을 요약합니다:\n",
        "\n",
        "- 자연어 처리의 기본 개념과 필요성\n",
        "- 텍스트 전처리 기법: 정제와 정규화\n",
        "- 토큰화 기법: 단어 토큰화와 문장 토큰화, 형태소 분석\n",
        "- 텍스트 임베딩: KONLPY를 활용한 한국어 처리"
      ],
      "metadata": {
        "id": "ssmLJVtDbjqu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}